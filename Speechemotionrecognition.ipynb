{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuQP7OR2OID21ldv0VjePl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boyaradhika123/ZomatoEDA/blob/main/Speechemotionrecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "D8WCtmUhuKki",
        "outputId": "b8924aba-8659-4667-ac4a-00ebb10b1177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base data path does not exist: /content/drive/MyDrive/extract_speech/TESS Toronto emotional speech set data\n",
            "Length of dataset: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2070079381.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, data_path, emotions, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.emotions = emotions\n",
        "        self.file_list = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        if not os.path.exists(data_path):\n",
        "            print(f\"Base data path does not exist: {data_path}\")\n",
        "            return\n",
        "\n",
        "        for idx, emotion in enumerate(emotions):\n",
        "            emotion_folders = [f'YAF_{emotion}', f'OAF_{emotion}']\n",
        "            for folder in emotion_folders:\n",
        "                folder_path = os.path.join(data_path, folder)\n",
        "                print(f\"Checking folder path: {folder_path}\")\n",
        "                if os.path.exists(folder_path):\n",
        "                    print(f\"Folder path exists: {folder_path}\")\n",
        "                    for file_name in os.listdir(folder_path):\n",
        "                        file_path = os.path.join(folder_path, file_name)\n",
        "                        self.file_list.append(file_path)\n",
        "                        self.labels.append(idx)\n",
        "                        print(f\"Processing file: {file_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_list[idx]\n",
        "        label = self.labels[idx]\n",
        "        y, sr = librosa.load(file_path, sr=16000)\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "        max_length = 128\n",
        "        pad_width = max_length - mel_spectrogram_db.shape[1]\n",
        "        if pad_width > 0:\n",
        "            mel_spectrogram_db = np.pad(mel_spectrogram_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mel_spectrogram_db = mel_spectrogram_db[:, :max_length]\n",
        "        mel_spectrogram_3ch = np.repeat(mel_spectrogram_db[np.newaxis, :, :], 3, axis=0)\n",
        "        return torch.tensor(mel_spectrogram_3ch, dtype=torch.float32), torch.tensor(label)\n",
        "\n",
        "class EmotionRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EmotionRecognitionModel, self).__init__()\n",
        "        self.vgg = models.vgg16(pretrained=True)\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.vgg.classifier[6] = nn.Linear(self.vgg.classifier[6].in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vgg(x)\n",
        "\n",
        "emotions = ['anger', 'disgust', 'fear', 'happiness', 'pleasant_surprise', 'sadness', 'neutral']\n",
        "data_path = '/content/drive/MyDrive/extract_speech/TESS Toronto emotional speech set data'\n",
        "dataset = EmotionDataset(data_path, emotions)\n",
        "\n",
        "print(f\"Length of dataset: {len(dataset)}\")\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "\n",
        "model = EmotionRecognitionModel(num_classes=len(emotions))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "total_train_correct = 0\n",
        "total_train_samples = 0\n",
        "total_val_correct = 0\n",
        "total_val_samples = 0\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        total_train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_accuracy = total_train_correct / total_train_samples\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            total_val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            total_val_samples += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = total_val_correct / total_val_samples\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "final_train_accuracy = total_train_correct / total_train_samples\n",
        "final_val_accuracy = total_val_correct / total_val_samples\n",
        "print(f\"Final Training Accuracy: {final_train_accuracy:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'emotion_recognition_model.pth')\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "model.load_state_dict(torch.load('emotion_recognition_model.pth'))\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "total_test_correct = 0\n",
        "total_test_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        total_test_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        total_test_samples += labels.size(0)\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = total_test_correct / total_test_samples\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "def predict_emotion(audio_path):\n",
        "\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    max_length = 128\n",
        "    pad_width = max_length - mel_spectrogram_db.shape[1]\n",
        "    if pad_width > 0:\n",
        "        mel_spectrogram_db = np.pad(mel_spectrogram_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mel_spectrogram_db = mel_spectrogram_db[:, :max_length]\n",
        "    mel_spectrogram_3ch = np.repeat(mel_spectrogram_db[np.newaxis, :, :], 3, axis=0)\n",
        "    input_tensor = torch.tensor(mel_spectrogram_3ch, dtype=torch.float32).unsqueeze(0)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "    return emotions[predicted_class]\n",
        "\n",
        "audio_file_path = '/content/drive/MyDrive/extract_speech/TESS Toronto emotional speech set data/OAF_Fear/OAF_bar_fear.wav'  # Replace with your audio file path\n",
        "predicted_emotion = predict_emotion(audio_file_path)\n",
        "print(f'Predicted Emotion: {predicted_emotion}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd1fd555"
      },
      "source": [
        "**Error Explanation:**\n",
        "\n",
        "The error `ValueError: num_samples should be a positive integer value, but got num_samples=0` occurred because the dataset is empty. This is indicated by `Length of dataset: 0` in the output. The `EmotionDataset` class was unable to find any audio files in the specified `data_path` and its subfolders.\n",
        "\n",
        "**Possible Causes:**\n",
        "\n",
        "1.  **Incorrect Data Path:** The path `/content/drive/MyDrive/extract_speech/TESS Toronto emotional speech set data` might be incorrect. Double-check the path to ensure it points to the correct location of your data on Google Drive.\n",
        "2.  **Google Drive Not Mounted:** Your Google Drive might not be correctly mounted in your Colab environment. You need to mount your drive to access files stored there.\n",
        "3.  **Folder Structure:** Ensure that the emotion folders (e.g., `YAF_anger`, `OAF_anger`) exist within the specified `data_path` and contain the audio files.\n",
        "\n",
        "**Troubleshooting Steps:**\n",
        "\n",
        "1.  **Mount Google Drive:** If you haven't already, mount your Google Drive by running the following code in a new cell:"
      ]
    }
  ]
}